{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["Copyright (c) Microsoft Corporation. All rights reserved.\n", "\n", "Licensed under the MIT License."], "cell_type": "markdown", "metadata": {}}, {"source": ["![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training/manage-runs/manage-runs.png)"], "cell_type": "markdown", "metadata": {}}, {"source": ["# Manage runs\n", "\n", "## Table of contents\n", "\n", "1. [Introduction](#Introduction)\n", "1. [Setup](#Setup)\n", "1. [Start, monitor and complete a run](#Start,-monitor-and-complete-a-run)\n", "1. [Add properties and tags](#Add-properties-and-tags)\n", "1. [Query properties and tags](#Query-properties-and-tags)\n", "1. [Start and query child runs](#Start-and-query-child-runs)\n", "1. [Cancel or fail runs](#Cancel-or-fail-runs)\n", "1. [Reproduce a run](#Reproduce-a-run)\n", "1. [Next steps](#Next-steps)"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Introduction\n", "\n", "When you're building enterprise-grade machine learning models, it is important to track, organize, monitor and reproduce your training runs. For example, you might want to trace the lineage behind a model deployed to production, and re-run the training experiment to troubleshoot issues. \n", "\n", "This notebooks shows examples how to use Azure Machine Learning services to manage your training runs."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Setup\n", "\n", "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace. Also, if you're new to Azure ML, we recommend that you go through [the tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml) first to learn the basic concepts.\n", "\n", "Let's first import required packages, check Azure ML SDK version, connect to your workspace and create an Experiment to hold the runs."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import azureml.core\n", "from azureml.core import Workspace, Experiment, Run\n", "from azureml.core import ScriptRunConfig\n", "\n", "print(azureml.core.VERSION)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["ws = Workspace.from_config()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["exp = Experiment(workspace=ws, name=\"explore-runs\")"], "outputs": [], "metadata": {}}, {"source": ["## Start, monitor and complete a run\n", "\n", "A run is an unit of execution, typically to train a model, but for other purposes as well, such as loading or transforming data. Runs are tracked by Azure ML service, and can be instrumented with metrics and artifact logging.\n", "\n", "A simplest way to start a run in your interactive Python session is to call *Experiment.start_logging* method. You can then log metrics from within the run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["notebook_run = exp.start_logging()\n", "\n", "notebook_run.log(name=\"message\", value=\"Hello from run!\")\n", "\n", "print(notebook_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["Use *get_status method* to get the status of the run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(notebook_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["Also, you can simply enter the run to get a link to Azure Portal details"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["notebook_run"], "outputs": [], "metadata": {}}, {"source": ["Method *get_details* gives you more details on the run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["notebook_run.get_details()"], "outputs": [], "metadata": {}}, {"source": ["Use *complete* method to end the run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["notebook_run.complete()\n", "print(notebook_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["You can also use Python's *with...as* pattern. The run will automatically complete when moving out of scope. This way you don't need to manually complete the run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["with exp.start_logging() as notebook_run:\n", "    notebook_run.log(name=\"message\", value=\"Hello from run!\")\n", "    print(\"Is it still running?\",notebook_run.get_status())\n", "    \n", "print(\"Has it completed?\",notebook_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["Next, let's look at submitting a run as a separate Python process. To keep the example simple, we submit the run on local computer. Other targets could include remote VMs and Machine Learning Compute clusters in your Azure ML Workspace.\n", "\n", "We use *hello.py* script as an example. To perform logging, we need to get a reference to the Run instance from within the scope of the script. We do this using *Run.get_context* method."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["!more hello.py"], "outputs": [], "metadata": {}}, {"source": ["Let's submit the run on a local computer. A standard pattern in Azure ML SDK is to create run configuration, and then use *Experiment.submit* method."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["run_config = ScriptRunConfig(source_directory='.', script='hello.py')\n", "\n", "local_script_run = exp.submit(run_config)"], "outputs": [], "metadata": {}}, {"source": ["You can view the status of the run as before"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(local_script_run.get_status())\n", "local_script_run"], "outputs": [], "metadata": {}}, {"source": ["Submitted runs have additional log files you can inspect using *get_details_with_logs*."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.get_details_with_logs()"], "outputs": [], "metadata": {}}, {"source": ["Use *wait_for_completion* method to block the local execution until remote run is complete."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.wait_for_completion(show_output=True)\n", "print(local_script_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["## Add properties and tags\n", "\n", "Properties and tags help you organize your runs. You can use them to describe, for example, who authored the run, what the results were, and what machine learning approach was used. And as you'll later learn, properties and tags can be used to query the history of your runs to find the important ones.\n", "\n", "For example, let's add \"author\" property to the run:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.add_properties({\"author\":\"azureml-user\"})\n", "print(local_script_run.get_properties())"], "outputs": [], "metadata": {}}, {"source": ["Properties are immutable. Once you assign a value it cannot be changed, making them useful as a permanent record for auditing purposes."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["try:\n", "    local_script_run.add_properties({\"author\":\"different-user\"})\n", "except Exception as e:\n", "    print(e)"], "outputs": [], "metadata": {}}, {"source": ["Tags on the other hand can be changed:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.tag(\"quality\", \"great run\")\n", "print(local_script_run.get_tags())"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.tag(\"quality\", \"fantastic run\")\n", "print(local_script_run.get_tags())"], "outputs": [], "metadata": {}}, {"source": ["You can also add a simple string tag. It appears in the tag dictionary with value of None"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.tag(\"worth another look\")\n", "print(local_script_run.get_tags())"], "outputs": [], "metadata": {}}, {"source": ["## Query properties and tags\n", "\n", "You can quary runs within an experiment that match specific properties and tags. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags={\"quality\":\"fantastic run\"}))"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags=\"worth another look\"))"], "outputs": [], "metadata": {}}, {"source": ["## Start and query child runs"], "cell_type": "markdown", "metadata": {}}, {"source": ["You can use child runs to group together related runs, for example different hyperparameter tuning iterations.\n", "\n", "Let's use *hello_with_children* script to create a batch of 5 child runs from within a submitted run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["!more hello_with_children.py"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["run_config = ScriptRunConfig(source_directory='.', script='hello_with_children.py')\n", "\n", "local_script_run = exp.submit(run_config)\n", "local_script_run.wait_for_completion(show_output=True)\n", "print(local_script_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["You can start child runs one by one. Note that this is less efficient than submitting a batch of runs, because each creation results in a network call.\n", "\n", "Child runs too complete automatically as they move out of scope."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["with exp.start_logging() as parent_run:\n", "    for c,count in enumerate(range(5)):\n", "        with parent_run.child_run() as child:\n", "            child.log(name=\"Hello from child run\", value=c)"], "outputs": [], "metadata": {}}, {"source": ["To query the child runs belonging to specific parent, use *get_children* method."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["list(parent_run.get_children())"], "outputs": [], "metadata": {}}, {"source": ["## Cancel or fail runs\n", "\n", "Sometimes, you realize that the run is not performing as intended, and you want to cancel it instead of waiting for it to complete.\n", "\n", "As an example, let's create a Python script with a delay in the middle."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["!more hello_with_delay.py"], "outputs": [], "metadata": {}}, {"source": ["You can use *cancel* method to cancel a run."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["run_config = ScriptRunConfig(source_directory='.', script='hello_with_delay.py')\n", "\n", "local_script_run = exp.submit(run_config)\n", "print(\"Did the run start?\",local_script_run.get_status())\n", "local_script_run.cancel()\n", "print(\"Did the run cancel?\",local_script_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["You can also mark an unsuccessful run as failed."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run = exp.submit(run_config)\n", "local_script_run.fail()\n", "print(local_script_run.get_status())"], "outputs": [], "metadata": {}}, {"source": ["## Reproduce a run\n", "\n", "When updating or troubleshooting on a model deployed to production, you sometimes need to revisit the original training run that produced the model. To help you with this, Azure ML service by default creates snapshots of your scripts a the time of run submission:\n", "\n", "You can use *restore_snapshot* to obtain a zip package of the latest snapshot of the script folder. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["local_script_run.restore_snapshot(path=\"snapshots\")"], "outputs": [], "metadata": {}}, {"source": ["You can then extract the zip package, examine the code, and submit your run again."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Next steps\n", "\n", " * To learn more about logging APIs, see [logging API notebook](./logging-api/logging-api.ipynb)\n", " * To learn more about remote runs, see [train on AML compute notebook](./train-on-amlcompute/train-on-amlcompute.ipynb)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.6 - AzureML", "name": "python3-azureml", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.5", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "authors": [{"name": "roastala"}]}}